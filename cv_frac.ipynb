{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataFilename = 'trainingSet.csv'\n",
    "testDataFilename = 'testSet.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_LIMIT = 8\n",
    "SAMPLE_LIMIT = 50\n",
    "class TreeNode(object):\n",
    "    def __init__(self, class_label, lineage):\n",
    "        self.val = class_label\n",
    "        self.lineage = lineage\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return ((self.left is None) or (self.right is None))\n",
    "\n",
    "    def expand(self, data, features, modelIdx, depth):\n",
    "        class_label = int(len(data[data['decision'] == 1]) > len(data[data['decision'] == 0]))\n",
    "        if modelIdx == 1 or modelIdx == 2:\n",
    "            available_features = [f for f in features if f not in self.lineage]\n",
    "        else:\n",
    "            rf_sample = [i for i in range(0, int(len(features)))]\n",
    "            np.random.shuffle(rf_sample)\n",
    "            rf_sample = rf_sample[0:int(np.sqrt(len(features)))]\n",
    "            rf_features = []\n",
    "            for i in rf_sample:\n",
    "                rf_features.append(features[i])\n",
    "            #print rf_features\n",
    "            available_features = [f for f in rf_features if f not in self.lineage]\n",
    "        if len(self.lineage) < depth - 1 and len(data) > 50:\n",
    "            ginis = {k: get_gini_gain(data, k) for k in available_features}\n",
    "            best_feature = max(ginis, key=ginis.get)\n",
    "            left_data = data[data[best_feature] == 0]\n",
    "            right_data = data[data[best_feature] == 1]\n",
    "            \n",
    "            self.val = best_feature\n",
    "            self.left = TreeNode(class_label, self.lineage + [best_feature])\n",
    "            self.left.expand(left_data, features, modelIdx, depth=depth)\n",
    "            self.right = TreeNode(class_label, self.lineage + [best_feature])\n",
    "            self.right.expand(right_data, features, modelIdx, depth=depth)\n",
    "        else:\n",
    "            self.val = class_label\n",
    "            \n",
    "def gini(data):\n",
    "    total = len(data)\n",
    "    pos = 1.0 * len(data[data['decision'] == 1])/(1.0 * total) if len(data[data['decision'] == 1]) > 0 else 0.0\n",
    "    neg = 1.0 * len(data[data['decision'] == 0])/(1.0 * total) if len(data[data['decision'] == 0]) > 0 else 0.0\n",
    "    return 1.0 - pos * pos - neg * neg\n",
    "\n",
    "def get_gini_gain(data, attr):\n",
    "    total = len(data)\n",
    "    pos = 1.0 * len(data[data[attr] == 1])/(1.0 * total) if len(data[data[attr] == 1]) > 0 else 0.0\n",
    "    neg = 1.0 * len(data[data[attr] == 0])/(1.0 * total) if len(data[data[attr] == 0]) > 0 else 0.0\n",
    "    gain = gini(data) - pos * gini(data[data[attr] == 1]) - neg * gini(data[data[attr] == 0])\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree(trainingSet, testSet, keys, d):\n",
    "    root = TreeNode(-1, [])\n",
    "    root.expand(trainingSet, keys, 1, d)\n",
    "    return root\n",
    "\n",
    "def predict(node, data):\n",
    "    if node.is_leaf():\n",
    "        return node.val\n",
    "    else:\n",
    "        if data[node.val] == 0:\n",
    "            return predict(node.left, data)\n",
    "        else:\n",
    "            return predict(node.right, data)\n",
    "\n",
    "def get_accuracy_dt(root, trainingSet, testSet):\n",
    "    count_train, total_train = 0, len(trainingSet)\n",
    "    count_test, total_test = 0, len(testSet)\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    predictions = []\n",
    "    Y = np.array(test_labels)\n",
    "\n",
    "    # Test accuracy\n",
    "    i = 0\n",
    "    for index, row in testSet.iterrows():\n",
    "        predictions.append(predict(root, row))\n",
    "        if int(predictions[i]) == Y[i]:\n",
    "            count_test += 1         \n",
    "        i += 1\n",
    "\n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy DT:', '%.2f' % test_accuracy\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(trainingSet, testSet, keys, d):\n",
    "    root_bagging = []\n",
    "    for i in range(30):\n",
    "        train = trainingSet.sample(frac = 1.0, replace=True)\n",
    "        root = TreeNode(-1, [])\n",
    "        root.expand(train, keys, 2, d)\n",
    "        root_bagging.append(root)\n",
    "    return root_bagging\n",
    "\n",
    "def get_accuracy_bagging(root, trainingSet, testSet):\n",
    "    count_train, total_train = 0, len(trainingSet)\n",
    "    count_test, total_test = 0, len(testSet)\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    predictions = [0 for i in range(len(testSet))]\n",
    "    Y = np.array(test_labels)\n",
    "\n",
    "    # Test accuracy\n",
    "    for r in root:\n",
    "        i = 0\n",
    "        for index, row in testSet.iterrows():\n",
    "            predictions[i] += int(predict(r, row))\n",
    "            i += 1\n",
    "\n",
    "    for i in range(len(testSet)):\n",
    "        if predictions[i] > 15 and Y[i] == 1:\n",
    "            count_test += 1\n",
    "        elif predictions[i] <= 15 and Y[i] == 0:\n",
    "            count_test += 1\n",
    "\n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy BT:', '%.2f' % test_accuracy\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForests(trainingSet, testSet, keys, d):\n",
    "    root_rf = []\n",
    "    for i in range(30):\n",
    "        train = trainingSet.sample(frac = 1.0, replace=True)\n",
    "        root = TreeNode(-1, [])\n",
    "        root.expand(train, keys, 3, d)\n",
    "        root_rf.append(root)\n",
    "        #get_accuracy_dt(root, trainingSet, testSet)\n",
    "    return root_rf\n",
    "    \n",
    "def get_accuracy_rf(root, trainingSet, testSet):\n",
    "    count_train, total_train = 0, len(trainingSet)\n",
    "    count_test, total_test = 0, len(testSet)\n",
    "    \n",
    "    train_labels = trainingSet['decision']\n",
    "    test_labels = testSet['decision']\n",
    "    \n",
    "    trainingSet = trainingSet.drop('decision', axis=1)\n",
    "    testSet = testSet.drop('decision', axis=1)\n",
    "    \n",
    "    predictions = [0 for i in range(len(testSet))]\n",
    "    Y = np.array(test_labels)\n",
    "\n",
    "    # Test accuracy\n",
    "    for r in root:\n",
    "        i = 0\n",
    "        for index, row in testSet.iterrows():\n",
    "            predictions[i] += int(predict(r, row))\n",
    "            i += 1\n",
    "\n",
    "    for i in range(len(testSet)):\n",
    "        if predictions[i] > 15 and Y[i] == 1:\n",
    "            count_test += 1\n",
    "        elif predictions[i] <= 15 and Y[i] == 0:\n",
    "            count_test += 1\n",
    "\n",
    "    test_accuracy = 1.0 * count_test/total_test\n",
    "    print 'Test Accuracy RF:', '%.2f' % test_accuracy\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = pd.read_csv(trainingDataFilename)\n",
    "testSet = pd.read_csv(testDataFilename)\n",
    "\n",
    "keys = trainingSet.keys()\n",
    "keys = keys.drop('decision')\n",
    "f = [0.05,0.075,0.1,0.15,0.2]\n",
    "trainingSet = trainingSet.sample(frac=1, random_state=18)\n",
    "\n",
    "df_kfold = []\n",
    "for i in range(10):\n",
    "    df_kfold.append(trainingSet[i*260:(i+1)*260])\n",
    "\n",
    "dt_res = {}\n",
    "bagging_res = {}\n",
    "rf_res = {}\n",
    "\n",
    "for t_frac in f:\n",
    "    dt_res[t_frac] = []\n",
    "    bagging_res[t_frac] = []\n",
    "    rf_res[t_frac] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "Test Accuracy DT: 0.65\n",
      "Test Accuracy BT: 0.67\n",
      "Test Accuracy RF: 0.65\n",
      "0.05\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.64\n",
      "Test Accuracy RF: 0.65\n",
      "0.05\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.73\n",
      "0.05\n",
      "Test Accuracy DT: 0.68\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.72\n",
      "0.05\n",
      "Test Accuracy DT: 0.67\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.72\n",
      "0.05\n",
      "Test Accuracy DT: 0.68\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.72\n",
      "0.05\n",
      "Test Accuracy DT: 0.67\n",
      "Test Accuracy BT: 0.73\n",
      "Test Accuracy RF: 0.67\n",
      "0.05\n",
      "Test Accuracy DT: 0.69\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.68\n",
      "0.075\n",
      "Test Accuracy DT: 0.68\n",
      "Test Accuracy BT: 0.68\n",
      "Test Accuracy RF: 0.64\n",
      "0.075\n",
      "Test Accuracy DT: 0.70\n",
      "Test Accuracy BT: 0.68\n",
      "Test Accuracy RF: 0.66\n",
      "0.075\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.75\n",
      "Test Accuracy RF: 0.77\n",
      "0.075\n",
      "Test Accuracy DT: 0.75\n",
      "Test Accuracy BT: 0.75\n",
      "Test Accuracy RF: 0.74\n",
      "0.075\n",
      "Test Accuracy DT: 0.67\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.69\n",
      "0.075\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.71\n",
      "Test Accuracy RF: 0.69\n",
      "0.075\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.71\n",
      "Test Accuracy RF: 0.69\n",
      "0.075\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.72\n",
      "0.075\n",
      "Test Accuracy DT: 0.66\n",
      "Test Accuracy BT: 0.67\n",
      "Test Accuracy RF: 0.70\n",
      "0.075\n",
      "Test Accuracy DT: 0.65\n",
      "Test Accuracy BT: 0.73\n",
      "Test Accuracy RF: 0.66\n",
      "0.1\n",
      "Test Accuracy DT: 0.66\n",
      "Test Accuracy BT: 0.68\n",
      "Test Accuracy RF: 0.68\n",
      "0.1\n",
      "Test Accuracy DT: 0.68\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.69\n",
      "0.1\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.74\n",
      "Test Accuracy RF: 0.75\n",
      "0.1\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.74\n",
      "Test Accuracy RF: 0.77\n",
      "0.1\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.71\n",
      "Test Accuracy RF: 0.71\n",
      "0.1\n",
      "Test Accuracy DT: 0.69\n",
      "Test Accuracy BT: 0.68\n",
      "Test Accuracy RF: 0.72\n",
      "0.1\n",
      "Test Accuracy DT: 0.73\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.73\n",
      "0.1\n",
      "Test Accuracy DT: 0.69\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.74\n",
      "0.1\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.68\n",
      "Test Accuracy RF: 0.68\n",
      "0.1\n",
      "Test Accuracy DT: 0.65\n",
      "Test Accuracy BT: 0.73\n",
      "Test Accuracy RF: 0.70\n",
      "0.15\n",
      "Test Accuracy DT: 0.70\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.69\n",
      "0.15\n",
      "Test Accuracy DT: 0.68\n",
      "Test Accuracy BT: 0.69\n",
      "Test Accuracy RF: 0.72\n",
      "0.15\n",
      "Test Accuracy DT: 0.73\n",
      "Test Accuracy BT: 0.75\n",
      "Test Accuracy RF: 0.76\n",
      "0.15\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.77\n",
      "Test Accuracy RF: 0.75\n",
      "0.15\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.75\n",
      "Test Accuracy RF: 0.68\n",
      "0.15\n",
      "Test Accuracy DT: 0.69\n",
      "Test Accuracy BT: 0.71\n",
      "Test Accuracy RF: 0.70\n",
      "0.15\n",
      "Test Accuracy DT: 0.73\n",
      "Test Accuracy BT: 0.78\n",
      "Test Accuracy RF: 0.77\n",
      "0.15\n",
      "Test Accuracy DT: 0.73\n",
      "Test Accuracy BT: 0.75\n",
      "Test Accuracy RF: 0.75\n",
      "0.15\n",
      "Test Accuracy DT: 0.69\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.70\n",
      "0.15\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.77\n",
      "Test Accuracy RF: 0.77\n",
      "0.2\n",
      "Test Accuracy DT: 0.70\n",
      "Test Accuracy BT: 0.71\n",
      "Test Accuracy RF: 0.70\n",
      "0.2\n",
      "Test Accuracy DT: 0.67\n",
      "Test Accuracy BT: 0.70\n",
      "Test Accuracy RF: 0.70\n",
      "0.2\n",
      "Test Accuracy DT: 0.74\n",
      "Test Accuracy BT: 0.73\n",
      "Test Accuracy RF: 0.75\n",
      "0.2\n",
      "Test Accuracy DT: 0.77\n",
      "Test Accuracy BT: 0.76\n",
      "Test Accuracy RF: 0.76\n",
      "0.2\n",
      "Test Accuracy DT: 0.73\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.70\n",
      "0.2\n",
      "Test Accuracy DT: 0.70\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.70\n",
      "0.2\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.77\n",
      "Test Accuracy RF: 0.76\n",
      "0.2\n",
      "Test Accuracy DT: 0.72\n",
      "Test Accuracy BT: 0.73\n",
      "Test Accuracy RF: 0.76\n",
      "0.2\n",
      "Test Accuracy DT: 0.71\n",
      "Test Accuracy BT: 0.72\n",
      "Test Accuracy RF: 0.72\n",
      "0.2\n",
      "Test Accuracy DT: 0.70\n",
      "Test Accuracy BT: 0.78\n",
      "Test Accuracy RF: 0.74\n"
     ]
    }
   ],
   "source": [
    "for t_frac in f:\n",
    "    print t_frac\n",
    "    for i in range(10):\n",
    "        # Partition the tarin and cv\n",
    "        train_set_df = []\n",
    "        \n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                train_set_df.append(df_kfold[j])\n",
    "            else:\n",
    "                test_set = df_kfold[j]\n",
    "        \n",
    "        train_set = pd.concat(train_set_df).sample(frac = t_frac, random_state=32)\n",
    "        \n",
    "        # Decision Tree\n",
    "        root = decisionTree(train_set, test_set, keys, DEPTH_LIMIT)\n",
    "        dt_res[t_frac].append(get_accuracy_dt(root, train_set, test_set))\n",
    "        \n",
    "        # Bagging\n",
    "        root = bagging(train_set, test_set, keys, DEPTH_LIMIT)\n",
    "        bagging_res[t_frac].append(get_accuracy_bagging(root, train_set, test_set))\n",
    "        \n",
    "        # Random Forest\n",
    "        root = randomForests(train_set, test_set, keys, DEPTH_LIMIT)\n",
    "        rf_res[t_frac].append(get_accuracy_rf(root, train_set, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nbc_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-46d98ed041ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnbc_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbc_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mnbc_stdrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbc_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlr_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nbc_res' is not defined"
     ]
    }
   ],
   "source": [
    "# Get avg accuracy\n",
    "nbc_avg = []\n",
    "lr_avg = []\n",
    "svm_avg = []\n",
    "\n",
    "# Get std error\n",
    "nbc_stdrr = []\n",
    "lr_stdrr = []\n",
    "svm_stdrr = []\n",
    "\n",
    "for i in f:\n",
    "    nbc_avg.append(np.mean(nbc_res[i], axis = 0))\n",
    "    nbc_stdrr.append(np.std(nbc_res[i], axis = 0)/np.sqrt(10))\n",
    "    lr_avg.append(np.mean(lr_res[i], axis = 0))\n",
    "    lr_stdrr.append(np.std(lr_res[i], axis = 0)/np.sqrt(10))\n",
    "    svm_avg.append(np.mean(svm_res[i], axis = 0))\n",
    "    svm_stdrr.append(np.std(svm_res[i], axis = 0)/np.sqrt(10))\n",
    "\n",
    "for i in range(len(f)):\n",
    "    f[i] *= 9 * 520\n",
    "\n",
    "#print lr_avg, svm_avg\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(f, nbc_avg, color='red', label='NBC')\n",
    "plt.plot(f, lr_avg, color='green', label='LR')\n",
    "plt.plot(f, svm_avg, color='blue', label='SVM')\n",
    "\n",
    "plt.errorbar(f, nbc_avg, nbc_stdrr, color='red')\n",
    "plt.errorbar(f, lr_avg, lr_stdrr, color='green')\n",
    "plt.errorbar(f, svm_avg, svm_stdrr, color='blue')\n",
    "\n",
    "plt.xlabel(\"t_frac\")\n",
    "plt.ylabel(\"accuracy\");\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('./cv.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
